{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7g3_qblo9Aj"
      },
      "source": [
        "# Finetuning Emot\n",
        "Emot is a Emotion Recognition dataset with 5 possible labels: `sadness`, `anger`, `love`, `fear`, `happy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TyOk1Nuo9Al"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "from nltk.tokenize import TweetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9i2ipNuo9Am"
      },
      "outputs": [],
      "source": [
        "# Forward function for sequence classification\n",
        "def forward_sequence_classification(\n",
        "    model, batch_data, i2w, is_test=False, device=\"cpu\", **kwargs\n",
        "):\n",
        "    # Unpack batch data\n",
        "    if len(batch_data) == 3:\n",
        "        (subword_batch, mask_batch, label_batch) = batch_data\n",
        "        token_type_batch = None\n",
        "    elif len(batch_data) == 4:\n",
        "        (subword_batch, mask_batch, token_type_batch, label_batch) = batch_data\n",
        "\n",
        "    # Prepare input & label\n",
        "    subword_batch = torch.LongTensor(subword_batch)\n",
        "    mask_batch = torch.FloatTensor(mask_batch)\n",
        "    token_type_batch = (\n",
        "        torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
        "    )\n",
        "    label_batch = torch.LongTensor(label_batch)\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        subword_batch = subword_batch.cuda()\n",
        "        mask_batch = mask_batch.cuda()\n",
        "        token_type_batch = (\n",
        "            token_type_batch.cuda() if token_type_batch is not None else None\n",
        "        )\n",
        "        label_batch = label_batch.cuda()\n",
        "\n",
        "    # Forward model\n",
        "    outputs = model(\n",
        "        subword_batch,\n",
        "        attention_mask=mask_batch,\n",
        "        token_type_ids=token_type_batch,\n",
        "        labels=label_batch,\n",
        "    )\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    # generate prediction & label list\n",
        "    list_hyp = []\n",
        "    list_label = []\n",
        "    hyp = torch.topk(logits, 1)[1]\n",
        "    for j in range(len(hyp)):\n",
        "        list_hyp.append(i2w[hyp[j].item()])\n",
        "        list_label.append(i2w[label_batch[j][0].item()])\n",
        "\n",
        "    return loss, list_hyp, list_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAWkkMDHo9Am"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "def document_sentiment_metrics_fn(list_hyp, list_label):\n",
        "    metrics = {}\n",
        "    metrics[\"ACC\"] = accuracy_score(list_label, list_hyp)\n",
        "    metrics[\"F1\"] = f1_score(list_label, list_hyp, average=\"macro\")\n",
        "    metrics[\"REC\"] = recall_score(list_label, list_hyp, average=\"macro\")\n",
        "    metrics[\"PRE\"] = precision_score(list_label, list_hyp, average=\"macro\")\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGFwYhl1o9An"
      },
      "outputs": [],
      "source": [
        "#####\n",
        "# Emotion Twitter\n",
        "#####\n",
        "class EmotionDetectionDataset(Dataset):\n",
        "    # Static constant variable\n",
        "    LABEL2INDEX = {\"sadness\": 0, \"anger\": 1, \"love\": 2, \"fear\": 3, \"happy\": 4}\n",
        "    INDEX2LABEL = {0: \"sadness\", 1: \"anger\", 2: \"love\", 3: \"fear\", 4: \"happy\"}\n",
        "    NUM_LABELS = 5\n",
        "\n",
        "    def load_dataset(self, path):\n",
        "        # Load dataset\n",
        "        dataset = pd.read_csv(path)\n",
        "        dataset[\"label\"] = dataset[\"label\"].apply(lambda sen: self.LABEL2INDEX[sen])\n",
        "        return dataset\n",
        "\n",
        "    def __init__(\n",
        "        self, dataset_path, tokenizer, no_special_token=False, *args, **kwargs\n",
        "    ):\n",
        "        self.data = self.load_dataset(dataset_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.no_special_token = no_special_token\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tweet, label = self.data.loc[index, \"tweet\"], self.data.loc[index, \"label\"]\n",
        "        subwords = self.tokenizer.encode(\n",
        "            tweet, add_special_tokens=not self.no_special_token\n",
        "        )\n",
        "        return np.array(subwords), np.array(label), tweet\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class EmotionDetectionDataLoader(DataLoader):\n",
        "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
        "        super(EmotionDetectionDataLoader, self).__init__(*args, **kwargs)\n",
        "        self.collate_fn = self._collate_fn\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def _collate_fn(self, batch):\n",
        "        batch_size = len(batch)\n",
        "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
        "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
        "\n",
        "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
        "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
        "        label_batch = np.full((batch_size, 1), -100, dtype=np.int64)\n",
        "\n",
        "        seq_list = []\n",
        "        for i, (subwords, label, raw_seq) in enumerate(batch):\n",
        "            subwords = subwords[:max_seq_len]\n",
        "            subword_batch[i, : len(subwords)] = subwords\n",
        "            mask_batch[i, : len(subwords)] = 1\n",
        "            label_batch[i] = label\n",
        "\n",
        "            seq_list.append(raw_seq)\n",
        "\n",
        "        return subword_batch, mask_batch, label_batch, seq_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVuRyGdGo9An"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# common functions\n",
        "###\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def count_param(module, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def metrics_to_string(metric_dict):\n",
        "    string_list = []\n",
        "    for key, value in metric_dict.items():\n",
        "        string_list.append('{}:{:.2f}'.format(key, value))\n",
        "    return ' '.join(string_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I64PVjsZo9An"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "set_seed(26092020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UYVJGV6o9Ao"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "--GXhOf0o9Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e492b8-bbbb-4601-979b-5fc501e522b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load Tokenizer and Config\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
        "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p2')\n",
        "config.num_labels = EmotionDetectionDataset.NUM_LABELS\n",
        "\n",
        "# Instantiate model\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p2', config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pX1b0qTo9Ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea06154-f581-4ab0-abf2-4613f0986eec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124445189"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "count_param(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZxnMleco9Ap"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkUNt671o9Ap"
      },
      "outputs": [],
      "source": [
        "train_dataset_path = \"https://raw.githubusercontent.com/IndoNLP/indonlu/master/dataset/emot_emotion-twitter/train_preprocess.csv\"\n",
        "valid_dataset_path = \"https://raw.githubusercontent.com/IndoNLP/indonlu/master/dataset/emot_emotion-twitter/valid_preprocess.csv\"\n",
        "test_dataset_path = \"https://raw.githubusercontent.com/IndoNLP/indonlu/master/dataset/emot_emotion-twitter/test_preprocess_masked_label.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAdE1umQo9Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef12647e-dfdd-4c9d-8e3e-3a488bbada3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "train_dataset = EmotionDetectionDataset(train_dataset_path, tokenizer, lowercase=True)\n",
        "valid_dataset = EmotionDetectionDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
        "test_dataset = EmotionDetectionDataset(test_dataset_path, tokenizer, lowercase=True)\n",
        "\n",
        "train_loader = EmotionDetectionDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)\n",
        "valid_loader = EmotionDetectionDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)\n",
        "test_loader = EmotionDetectionDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9Jppe8Uo9Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30326d9-a72a-48e5-dafc-2fcd827be253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sadness': 0, 'anger': 1, 'love': 2, 'fear': 3, 'happy': 4}\n",
            "{0: 'sadness', 1: 'anger', 2: 'love', 3: 'fear', 4: 'happy'}\n"
          ]
        }
      ],
      "source": [
        "w2i, i2w = EmotionDetectionDataset.LABEL2INDEX, EmotionDetectionDataset.INDEX2LABEL\n",
        "print(w2i)\n",
        "print(i2w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srYT-Wm9o9Ap"
      },
      "source": [
        "# Test model on sample sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj6iFUVdo9Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca5dcaa-b07a-49df-e6e8-6bfcc7ea656f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : sadness (25.796%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzZkoc9So9Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093f7c20-be35-4da9-ba91-15757983c366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Budi pergi ke pondok indah mall membeli cakwe | Label : sadness (27.013%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Budi pergi ke pondok indah mall membeli cakwe'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGbXJyPro9Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e93431c-9947-4a3d-afa4-1bccfe4f02ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Dasar anak sialan!! Kurang ajar!! | Label : sadness (27.658%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Dasar anak sialan!! Kurang ajar!!'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KswZhW9so9Ap"
      },
      "source": [
        "# Fine Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps6OQOQCo9Ap"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=5e-6)\n",
        "\n",
        "model = model.cuda(device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCvUany8o9Aq",
        "outputId": "dcab076e-b682-4ad4-c1d2-709676762d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 1) TRAIN LOSS:1.3361 LR:0.00000500: 100%|██████████| 111/111 [00:49<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 1) TRAIN LOSS:1.3361 ACC:0.46 F1:0.42 REC:0.42 PRE:0.48 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.0409 ACC:0.59 F1:0.58 REC:0.58 PRE:0.61: 100%|██████████| 14/14 [00:03<00:00,  4.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 1) VALID LOSS:1.0409 ACC:0.59 F1:0.58 REC:0.58 PRE:0.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 2) TRAIN LOSS:0.8947 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 2) TRAIN LOSS:0.8947 ACC:0.69 F1:0.68 REC:0.68 PRE:0.69 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:0.8040 ACC:0.68 F1:0.68 REC:0.68 PRE:0.68: 100%|██████████| 14/14 [00:03<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 2) VALID LOSS:0.8040 ACC:0.68 F1:0.68 REC:0.68 PRE:0.68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 3) TRAIN LOSS:0.6346 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.6346 ACC:0.78 F1:0.78 REC:0.78 PRE:0.79 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:0.7765 ACC:0.70 F1:0.70 REC:0.71 PRE:0.72: 100%|██████████| 14/14 [00:03<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 3) VALID LOSS:0.7765 ACC:0.70 F1:0.70 REC:0.71 PRE:0.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 4) TRAIN LOSS:0.4890 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 4) TRAIN LOSS:0.4890 ACC:0.84 F1:0.84 REC:0.84 PRE:0.84 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:0.7656 ACC:0.71 F1:0.71 REC:0.72 PRE:0.72: 100%|██████████| 14/14 [00:03<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 4) VALID LOSS:0.7656 ACC:0.71 F1:0.71 REC:0.72 PRE:0.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 5) TRAIN LOSS:0.3817 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 5) TRAIN LOSS:0.3817 ACC:0.87 F1:0.88 REC:0.87 PRE:0.88 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:0.8170 ACC:0.72 F1:0.73 REC:0.72 PRE:0.73: 100%|██████████| 14/14 [00:03<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 5) VALID LOSS:0.8170 ACC:0.72 F1:0.73 REC:0.72 PRE:0.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 6) TRAIN LOSS:0.2637 LR:0.00000500: 100%|██████████| 111/111 [00:49<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 6) TRAIN LOSS:0.2637 ACC:0.92 F1:0.92 REC:0.92 PRE:0.93 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:0.8743 ACC:0.71 F1:0.71 REC:0.71 PRE:0.72: 100%|██████████| 14/14 [00:03<00:00,  3.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 6) VALID LOSS:0.8743 ACC:0.71 F1:0.71 REC:0.71 PRE:0.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 7) TRAIN LOSS:0.1886 LR:0.00000500: 100%|██████████| 111/111 [00:49<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 7) TRAIN LOSS:0.1886 ACC:0.95 F1:0.95 REC:0.94 PRE:0.95 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:0.9420 ACC:0.71 F1:0.72 REC:0.72 PRE:0.73: 100%|██████████| 14/14 [00:03<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 7) VALID LOSS:0.9420 ACC:0.71 F1:0.72 REC:0.72 PRE:0.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 8) TRAIN LOSS:0.1246 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 8) TRAIN LOSS:0.1246 ACC:0.97 F1:0.97 REC:0.97 PRE:0.97 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.0525 ACC:0.70 F1:0.71 REC:0.70 PRE:0.71: 100%|██████████| 14/14 [00:03<00:00,  4.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 8) VALID LOSS:1.0525 ACC:0.70 F1:0.71 REC:0.70 PRE:0.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 9) TRAIN LOSS:0.0783 LR:0.00000500: 100%|██████████| 111/111 [00:49<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 9) TRAIN LOSS:0.0783 ACC:0.98 F1:0.98 REC:0.98 PRE:0.98 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.1079 ACC:0.70 F1:0.71 REC:0.71 PRE:0.72: 100%|██████████| 14/14 [00:03<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 9) VALID LOSS:1.1079 ACC:0.70 F1:0.71 REC:0.71 PRE:0.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 10) TRAIN LOSS:0.0634 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 10) TRAIN LOSS:0.0634 ACC:0.99 F1:0.99 REC:0.99 PRE:0.99 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.2022 ACC:0.70 F1:0.71 REC:0.70 PRE:0.72: 100%|██████████| 14/14 [00:03<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 10) VALID LOSS:1.2022 ACC:0.70 F1:0.71 REC:0.70 PRE:0.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 11) TRAIN LOSS:0.0663 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 11) TRAIN LOSS:0.0663 ACC:0.98 F1:0.98 REC:0.98 PRE:0.99 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.2357 ACC:0.69 F1:0.70 REC:0.70 PRE:0.70: 100%|██████████| 14/14 [00:03<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 11) VALID LOSS:1.2357 ACC:0.69 F1:0.70 REC:0.70 PRE:0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 12) TRAIN LOSS:0.0361 LR:0.00000500: 100%|██████████| 111/111 [00:49<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 12) TRAIN LOSS:0.0361 ACC:0.99 F1:0.99 REC:0.99 PRE:0.99 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.3118 ACC:0.69 F1:0.69 REC:0.70 PRE:0.69: 100%|██████████| 14/14 [00:03<00:00,  3.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 12) VALID LOSS:1.3118 ACC:0.69 F1:0.69 REC:0.70 PRE:0.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 13) TRAIN LOSS:0.0237 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 13) TRAIN LOSS:0.0237 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.3051 ACC:0.68 F1:0.69 REC:0.69 PRE:0.69: 100%|██████████| 14/14 [00:03<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 13) VALID LOSS:1.3051 ACC:0.68 F1:0.69 REC:0.69 PRE:0.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 14) TRAIN LOSS:0.0204 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 14) TRAIN LOSS:0.0204 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.3673 ACC:0.68 F1:0.69 REC:0.69 PRE:0.69: 100%|██████████| 14/14 [00:03<00:00,  4.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 14) VALID LOSS:1.3673 ACC:0.68 F1:0.69 REC:0.69 PRE:0.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 15) TRAIN LOSS:0.0224 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 15) TRAIN LOSS:0.0224 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.3921 ACC:0.68 F1:0.69 REC:0.69 PRE:0.69: 100%|██████████| 14/14 [00:03<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 15) VALID LOSS:1.3921 ACC:0.68 F1:0.69 REC:0.69 PRE:0.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 16) TRAIN LOSS:0.0153 LR:0.00000500: 100%|██████████| 111/111 [00:49<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 16) TRAIN LOSS:0.0153 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.4660 ACC:0.68 F1:0.69 REC:0.69 PRE:0.69: 100%|██████████| 14/14 [00:03<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 16) VALID LOSS:1.4660 ACC:0.68 F1:0.69 REC:0.69 PRE:0.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 17) TRAIN LOSS:0.0142 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 17) TRAIN LOSS:0.0142 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.4602 ACC:0.69 F1:0.70 REC:0.70 PRE:0.70: 100%|██████████| 14/14 [00:03<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 17) VALID LOSS:1.4602 ACC:0.69 F1:0.70 REC:0.70 PRE:0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 18) TRAIN LOSS:0.0099 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 18) TRAIN LOSS:0.0099 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.5294 ACC:0.68 F1:0.69 REC:0.69 PRE:0.71: 100%|██████████| 14/14 [00:03<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 18) VALID LOSS:1.5294 ACC:0.68 F1:0.69 REC:0.69 PRE:0.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 19) TRAIN LOSS:0.0136 LR:0.00000500: 100%|██████████| 111/111 [00:49<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 19) TRAIN LOSS:0.0136 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.5607 ACC:0.69 F1:0.70 REC:0.70 PRE:0.70: 100%|██████████| 14/14 [00:03<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 19) VALID LOSS:1.5607 ACC:0.69 F1:0.70 REC:0.70 PRE:0.70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/111 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "(Epoch 20) TRAIN LOSS:0.0110 LR:0.00000500: 100%|██████████| 111/111 [00:48<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 20) TRAIN LOSS:0.0110 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VALID LOSS:1.5008 ACC:0.70 F1:0.71 REC:0.71 PRE:0.72: 100%|██████████| 14/14 [00:03<00:00,  4.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 20) VALID LOSS:1.5008 ACC:0.70 F1:0.71 REC:0.71 PRE:0.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "n_epochs = 20\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch_data in enumerate(train_pbar):\n",
        "        # Forward model\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        # Update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss = total_train_loss + tr_loss\n",
        "\n",
        "        # Calculate metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "\n",
        "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
        "            total_train_loss/(i+1), get_lr(optimizer)))\n",
        "\n",
        "    # Calculate train metric\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
        "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
        "\n",
        "    # Evaluate on validation\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss, total_correct, total_labels = 0, 0, 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(pbar):\n",
        "        batch_seq = batch_data[-1]\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        # Calculate total loss\n",
        "        valid_loss = loss.item()\n",
        "        total_loss = total_loss + valid_loss\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
        "\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
        "        total_loss/(i+1), metrics_to_string(metrics)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKBnpgJ1o9Aq",
        "outputId": "0250b405-ab8b-42e0-80c7-f6dd6b3e5cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/14 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|██████████| 14/14 [00:03<00:00,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     index    label\n",
            "0        0     love\n",
            "1        1     fear\n",
            "2        2     fear\n",
            "3        3    happy\n",
            "4        4    happy\n",
            "..     ...      ...\n",
            "435    435  sadness\n",
            "436    436  sadness\n",
            "437    437     fear\n",
            "438    438  sadness\n",
            "439    439    happy\n",
            "\n",
            "[440 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "\n",
        "# Save prediction\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('pred.txt', index=False)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FsfV-Syo9Aq"
      },
      "source": [
        "# Test fine-tuned model on sample sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(\"emot-fine-tuned.pth\")\n",
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IivzonD0Vkj",
        "outputId": "bf0c2ed0-1b91-42f3-fe12-10619896d202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtE8qNBao9Aq",
        "outputId": "d324e337-a776-4423-94aa-8b2baa450b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : happy (99.458%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4vihPq-o9Aq",
        "outputId": "f0f39903-6117-4073-c015-864ea9b20121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Budi pergi ke pondok indah mall membeli cakwe | Label : sadness (99.281%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Budi pergi ke pondok indah mall membeli cakwe'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnBG_VFVo9Aq",
        "outputId": "27c7ed9f-045d-4af4-d5ec-109ca5e7015a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Dasar anak sialan!! Kurang ajar!! | Label : anger (99.868%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Dasar anak sialan!! Kurang ajar!!'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lthAmQ2To9Aq"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"emot-fine-tuned.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STLCNIrAzX9M",
        "outputId": "29199883-2ca8-4b18-cff1-751ef4526784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "Aw1P9vgQzteS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"tweet.pth\",\"/content/drive/MyDrive/bert_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CRTIJYtnzyx2",
        "outputId": "f3024ec6-2802-48f2-ad12-931e088394ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/bert_model/tweet.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}